{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_mt5_ja",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1gClX198c-aB5gXsvoCb7jsDaC4xsAzLU",
      "authorship_tag": "ABX9TyMNfDuLXQc8p+XC8ZKOiSUR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0x71d3/hf-mt5-ja/blob/main/train_mt5_ja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1QFTFwFvjrn"
      },
      "source": [
        "tutorial\r\n",
        "\r\n",
        "https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn5T3suAc-yp"
      },
      "source": [
        "blog\r\n",
        "\r\n",
        "https://huggingface.co/blog/how-to-generate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeZm4k_Nfvez"
      },
      "source": [
        "mt5\r\n",
        "\r\n",
        "https://huggingface.co/transformers/v4.0.0/model_doc/mt5.html#mt5forconditionalgeneration\r\n",
        "\r\n",
        "https://huggingface.co/transformers/v4.0.0/model_doc/t5.html#t5tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0jLsSR7vUuo"
      },
      "source": [
        "install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OkMhPy1vKyV"
      },
      "source": [
        "!pip install transformers==4.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te0IBsaTil6p"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsz_oQyvvXGI"
      },
      "source": [
        "import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAKOr_ldtK6L"
      },
      "source": [
        "import csv\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "\r\n",
        "from transformers import MT5ForConditionalGeneration, T5Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTuBf440vmaZ"
      },
      "source": [
        "GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVFQjezm6OyM"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9o-WFSutRqf"
      },
      "source": [
        "from torch import cuda\r\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxteLGsPtVIJ"
      },
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_hsBu0NtWzj"
      },
      "source": [
        "class CustomDataset(Dataset):\r\n",
        "    def __init__(self, path, tokenizer, max_len):\r\n",
        "        sources = []\r\n",
        "        targets = []\r\n",
        "        with open(path, newline='') as f:\r\n",
        "            reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\r\n",
        "            for row in reader:\r\n",
        "                sources.append(row[0])\r\n",
        "                targets.append(row[1])\r\n",
        "                \r\n",
        "        self.batch = tokenizer.prepare_seq2seq_batch(\r\n",
        "            src_texts=sources,\r\n",
        "            tgt_texts=targets,\r\n",
        "            max_length=max_len,\r\n",
        "            return_tensors='pt'\r\n",
        "        )\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.batch['input_ids'].size(0)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        input_ids = self.batch['input_ids'][index]\r\n",
        "        attention_mask = self.batch['attention_mask'][index]\r\n",
        "        labels = self.batch['labels'][index]\r\n",
        "\r\n",
        "        return {\r\n",
        "            'input_ids': input_ids.to(dtype=torch.long), \r\n",
        "            'attention_mask': attention_mask.to(dtype=torch.long), \r\n",
        "            'labels': labels.to(dtype=torch.long),\r\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaFbxEzft1N2"
      },
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRDSMZ6At17A"
      },
      "source": [
        "def train(epoch, tokenizer, model, device, loader, optimizer):\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    for i, batch in enumerate(loader):\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        input_ids = batch['input_ids'].to(device, dtype=torch.long)\r\n",
        "        attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\r\n",
        "        labels = batch['labels'].to(device, dtype=torch.long)\r\n",
        "\r\n",
        "        # outputs = model(**batch)\r\n",
        "        outputs = model(\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            labels=labels\r\n",
        "        )\r\n",
        "        loss = outputs.loss\r\n",
        "\r\n",
        "        if i % 50 == 0:\r\n",
        "            print(f'Epoch: {epoch}, Loss: {loss.item()}')\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c2ghxpqt7yR"
      },
      "source": [
        "validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q__ZIAot7PC"
      },
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    predictions = []\r\n",
        "    actuals = []\r\n",
        "    with torch.no_grad():\r\n",
        "        for i, batch in enumerate(loader):\r\n",
        "            input_ids = batch['input_ids'].to(device, dtype=torch.long)\r\n",
        "            attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\r\n",
        "            labels = batch['labels'].to(device, dtype=torch.long)\r\n",
        "\r\n",
        "            generated_ids = model.generate(\r\n",
        "                input_ids=input_ids,\r\n",
        "                attention_mask=attention_mask, \r\n",
        "                max_length=50, \r\n",
        "                num_beams=5, \r\n",
        "                no_repeat_ngram_size=2, \r\n",
        "                early_stopping=True\r\n",
        "            )\r\n",
        "            preds = [\r\n",
        "                tokenizer.decode(generated_id, skip_special_tokens=True)\r\n",
        "                for generated_id in generated_ids\r\n",
        "            ]\r\n",
        "            target = [\r\n",
        "                tokenizer.decode(label, skip_special_tokens=True)\r\n",
        "                for label in labels\r\n",
        "            ]\r\n",
        "            \r\n",
        "            if i % 10 == 0:\r\n",
        "                print(f'Completed {i}')\r\n",
        "\r\n",
        "            predictions.extend(preds)\r\n",
        "            actuals.extend(target)\r\n",
        "    return predictions, actuals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2x3meaEY7Du"
      },
      "source": [
        "main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33t6RLhxuD_T"
      },
      "source": [
        "def main():\r\n",
        "    TRAIN_BATCH_SIZE = 2\r\n",
        "    VALID_BATCH_SIZE = 4\r\n",
        "    TRAIN_EPOCHS = 2\r\n",
        "    # VAL_EPOCHS = 1\r\n",
        "    LEARNING_RATE = 1e-4\r\n",
        "    SEED = 42\r\n",
        "    MAX_LEN = 32\r\n",
        "\r\n",
        "    torch.manual_seed(SEED)  # pytorch random seed\r\n",
        "    np.random.seed(SEED)  # numpy random seed\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-small\")\r\n",
        "\r\n",
        "    train_set = CustomDataset(\r\n",
        "        'drive/My Drive/stc-jpn/train.tsv',\r\n",
        "        tokenizer,\r\n",
        "        MAX_LEN\r\n",
        "    )\r\n",
        "    val_set = CustomDataset(\r\n",
        "        'drive/My Drive/stc-jpn/val.tsv',\r\n",
        "        tokenizer,\r\n",
        "        MAX_LEN\r\n",
        "    )\r\n",
        "\r\n",
        "    train_loader = DataLoader(\r\n",
        "        train_set,\r\n",
        "        batch_size=TRAIN_BATCH_SIZE,\r\n",
        "        shuffle=True,\r\n",
        "        num_workers=0\r\n",
        "    )\r\n",
        "    val_loader = DataLoader(\r\n",
        "        val_set,\r\n",
        "        batch_size=VALID_BATCH_SIZE,\r\n",
        "        shuffle=False,\r\n",
        "        num_workers=0\r\n",
        "    )\r\n",
        "\r\n",
        "    model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\r\n",
        "    model = model.to(device)\r\n",
        "\r\n",
        "    optimizer = torch.optim.Adam(\r\n",
        "        params=model.parameters(),\r\n",
        "        lr=LEARNING_RATE\r\n",
        "    )\r\n",
        "\r\n",
        "    for epoch in range(TRAIN_EPOCHS):\r\n",
        "        train(epoch, tokenizer, model, device, train_loader, optimizer)\r\n",
        "\r\n",
        "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\r\n",
        "    with open('predictions.tsv', 'w') as f:\r\n",
        "        for prediction, actual in zip(predictions, actuals):\r\n",
        "            f.write(prediction + '\\t' + actual + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dtWjao2uYIG"
      },
      "source": [
        "if __name__ == '__main__':\r\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21zTLf0vmCOy"
      },
      "source": [
        "copy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h12M3h6mBm4"
      },
      "source": [
        "!cp predictions.tsv drive/My\\ Drive/stc-jpn/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}