{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mt5_ja.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbgZEDeuUsbt"
      },
      "source": [
        "copy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EAMgHq_Up0_"
      },
      "source": [
        "!cp -r drive/My\\ Drive/stc-jpn ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYt41D7PUqIe"
      },
      "source": [
        "install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7vWL3s5fH4_"
      },
      "source": [
        "!pip install pytorch-lightning==1.1.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wIoAfTofHmc"
      },
      "source": [
        "!pip install transformers==4.1.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S11vDg_xUfVD"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPHl9m4QUnSy"
      },
      "source": [
        "main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPQJrVHOCgQa"
      },
      "source": [
        "import argparse\r\n",
        "import csv\r\n",
        "import os\r\n",
        "import shutil\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from pytorch_lightning import LightningModule, Trainer, seed_everything\r\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\r\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\r\n",
        "from transformers import (\r\n",
        "    T5Tokenizer,\r\n",
        "    MT5ForConditionalGeneration,\r\n",
        "    AdamW,\r\n",
        "    get_linear_schedule_with_warmup\r\n",
        ")\r\n",
        "from transformers.models.bart.modeling_bart import shift_tokens_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DmEBp_dChIc"
      },
      "source": [
        "class DialogueDataset(Dataset):\r\n",
        "    def __init__(self, data_dir, split, tokenizer, max_length):\r\n",
        "        src_texts = []\r\n",
        "        tgt_texts = []\r\n",
        "        with open(os.path.join(data_dir, split + '.tsv')) as f:\r\n",
        "            reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\r\n",
        "            for row in reader:\r\n",
        "                src_texts.append(row[0])\r\n",
        "                tgt_texts.append(row[1])\r\n",
        "        \r\n",
        "        self.batch = tokenizer.prepare_seq2seq_batch(\r\n",
        "            src_texts=src_texts,\r\n",
        "            tgt_texts=tgt_texts,\r\n",
        "            max_length=max_length,\r\n",
        "            return_tensors='pt'\r\n",
        "        )\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.batch['input_ids'].size(0)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        input_ids = self.batch['input_ids'][index]\r\n",
        "        attention_mask = self.batch['attention_mask'][index]\r\n",
        "        labels = self.batch['labels'][index]\r\n",
        "\r\n",
        "        return {\r\n",
        "            'input_ids': input_ids,\r\n",
        "            'attention_mask': attention_mask,\r\n",
        "            'labels': labels\r\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNF55lw1Cs8x"
      },
      "source": [
        "def label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=-100):\r\n",
        "    '''From fairseq'''\r\n",
        "    if target.dim() == lprobs.dim() - 1:\r\n",
        "        target = target.unsqueeze(-1)\r\n",
        "    nll_loss = -lprobs.gather(dim=-1, index=target)\r\n",
        "    smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\r\n",
        "    if ignore_index is not None:\r\n",
        "        pad_mask = target.eq(ignore_index)\r\n",
        "        nll_loss.masked_fill_(pad_mask, 0.0)\r\n",
        "        smooth_loss.masked_fill_(pad_mask, 0.0)\r\n",
        "    else:\r\n",
        "        nll_loss = nll_loss.squeeze(-1)\r\n",
        "        smooth_loss = smooth_loss.squeeze(-1)\r\n",
        "\r\n",
        "    nll_loss = nll_loss.sum()  # mean()? Scared to break other math.\r\n",
        "    smooth_loss = smooth_loss.sum()\r\n",
        "    eps_i = epsilon / lprobs.size(-1)\r\n",
        "    loss = (1.0 - epsilon) * nll_loss + eps_i * smooth_loss\r\n",
        "    return loss, nll_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL6mXhm8Cys0"
      },
      "source": [
        "class MT5Trainer(LightningModule):\r\n",
        "    def __init__(self, params):\r\n",
        "        super().__init__()\r\n",
        "        self.save_hyperparameters(params)\r\n",
        "\r\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained('google/mt5-small')\r\n",
        "        self.model = MT5ForConditionalGeneration.from_pretrained(\r\n",
        "            'google/mt5-small'\r\n",
        "        )\r\n",
        "\r\n",
        "        # loader\r\n",
        "        dataset = DialogueDataset(\r\n",
        "            data_dir=self.hparams.data_dir,\r\n",
        "            split='train',\r\n",
        "            tokenizer=self.tokenizer,\r\n",
        "            max_length=self.hparams.max_length\r\n",
        "        )\r\n",
        "        self.train_loader = DataLoader(\r\n",
        "            dataset=dataset,\r\n",
        "            batch_size=self.hparams.train_batch_size,\r\n",
        "            shuffle=True\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids):\r\n",
        "        return self.model(\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            decoder_input_ids=decoder_input_ids\r\n",
        "        )\r\n",
        "\r\n",
        "    def training_step(self, batch, batch_idx):\r\n",
        "        pad_token_id = self.tokenizer.pad_token_id\r\n",
        "\r\n",
        "        input_ids = batch['input_ids']\r\n",
        "        attention_mask = batch['attention_mask']\r\n",
        "        labels = batch['labels']\r\n",
        "\r\n",
        "        decoder_input_ids = shift_tokens_right(labels, pad_token_id)\r\n",
        "\r\n",
        "        outputs = self(\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            decoder_input_ids=decoder_input_ids\r\n",
        "        )\r\n",
        "        \r\n",
        "        logits = outputs[0]\r\n",
        "        lprobs = torch.nn.functional.log_softmax(logits, dim=-1)\r\n",
        "        loss, nll_loss = label_smoothed_nll_loss(\r\n",
        "            lprobs=lprobs,\r\n",
        "            target=labels,\r\n",
        "            epsilon=self.hparams.label_smoothing,\r\n",
        "            ignore_index=pad_token_id\r\n",
        "        )\r\n",
        "\r\n",
        "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\r\n",
        "        return loss\r\n",
        "    \r\n",
        "    def validation_step(self, batch, batch_idx):\r\n",
        "        pad_token_id = self.tokenizer.pad_token_id\r\n",
        "\r\n",
        "        input_ids = batch['input_ids']\r\n",
        "        attention_mask = batch['attention_mask']\r\n",
        "        labels = batch['labels']\r\n",
        "\r\n",
        "        decoder_input_ids = shift_tokens_right(labels, pad_token_id)\r\n",
        "\r\n",
        "        outputs = self(\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            decoder_input_ids=decoder_input_ids\r\n",
        "        )\r\n",
        "        \r\n",
        "        logits = outputs[0]\r\n",
        "        lprobs = torch.nn.functional.log_softmax(logits, dim=-1)\r\n",
        "        loss, nll_loss = label_smoothed_nll_loss(\r\n",
        "            lprobs=lprobs,\r\n",
        "            target=labels,\r\n",
        "            epsilon=self.hparams.label_smoothing,\r\n",
        "            ignore_index=pad_token_id\r\n",
        "        )\r\n",
        "\r\n",
        "        self.log('val_loss', loss, prog_bar=True)\r\n",
        "    \r\n",
        "    def test_step(self, batch, batch_idx):\r\n",
        "        input_ids = batch['input_ids']\r\n",
        "        attention_mask = batch['attention_mask']\r\n",
        "\r\n",
        "        # https://huggingface.co/blog/how-to-generate\r\n",
        "        beam_outputs = self.model.generate(\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            max_length=50,\r\n",
        "            num_beams=5,\r\n",
        "            no_repeat_ngram_size=2,\r\n",
        "            early_stopping=True\r\n",
        "        )\r\n",
        "\r\n",
        "        preds = [\r\n",
        "            self.tokenizer.decode(beam_output, skip_special_tokens=True)\r\n",
        "            for beam_output in beam_outputs\r\n",
        "        ]\r\n",
        "        return preds\r\n",
        "\r\n",
        "    def test_epoch_end(self, outputs):\r\n",
        "        with open(os.path.join(self.hparams.output_dir, 'preds.txt'), 'w') as f:\r\n",
        "            for output in outputs:\r\n",
        "                f.write('\\n'.join(output) + '\\n')\r\n",
        "\r\n",
        "    def configure_optimizers(self):\r\n",
        "        # optimizer\r\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\r\n",
        "        optimizer_grouped_parameters = [\r\n",
        "            {\r\n",
        "                'params': [\r\n",
        "                    p for n, p in self.model.named_parameters()\r\n",
        "                    if not any(nd in n for nd in no_decay)\r\n",
        "                ],\r\n",
        "                'weight_decay': self.hparams.weight_decay\r\n",
        "            },\r\n",
        "            {\r\n",
        "                'params': [\r\n",
        "                    p for n, p in self.model.named_parameters()\r\n",
        "                    if any(nd in n for nd in no_decay)\r\n",
        "                ],\r\n",
        "                'weight_decay': 0.0\r\n",
        "            },\r\n",
        "        ]\r\n",
        "        betas = tuple(map(float, self.hparams.adam_betas[1:-1].split(',')))\r\n",
        "        optimizer = AdamW(\r\n",
        "            optimizer_grouped_parameters,\r\n",
        "            betas=betas,\r\n",
        "            eps=self.hparams.adam_eps,\r\n",
        "            lr=self.hparams.lr\r\n",
        "        )\r\n",
        "\r\n",
        "        # scheduler\r\n",
        "        num_training_steps = (\r\n",
        "            len(self.train_loader)\r\n",
        "            // self.hparams.accumulate_grad_batches\r\n",
        "            * self.hparams.max_epochs\r\n",
        "        )\r\n",
        "        lr_scheduler = get_linear_schedule_with_warmup(\r\n",
        "            optimizer,\r\n",
        "            num_warmup_steps=self.hparams.num_warmup_steps,\r\n",
        "            num_training_steps=num_training_steps\r\n",
        "        )\r\n",
        "        lr_dict = {'scheduler': lr_scheduler, 'interval': 'step'}\r\n",
        "\r\n",
        "        return [optimizer], [lr_dict]\r\n",
        "\r\n",
        "    def train_dataloader(self):\r\n",
        "        return self.train_loader\r\n",
        "\r\n",
        "    def val_dataloader(self):\r\n",
        "        dataset = DialogueDataset(\r\n",
        "            data_dir=self.hparams.data_dir,\r\n",
        "            split='val',\r\n",
        "            tokenizer=self.tokenizer,\r\n",
        "            max_length=self.hparams.max_length\r\n",
        "        )\r\n",
        "        loader = DataLoader(\r\n",
        "            dataset=dataset,\r\n",
        "            batch_size=self.hparams.val_batch_size\r\n",
        "        )\r\n",
        "        return loader\r\n",
        "\r\n",
        "    def test_dataloader(self):\r\n",
        "        dataset = DialogueDataset(\r\n",
        "            data_dir=self.hparams.data_dir,\r\n",
        "            split='test',\r\n",
        "            tokenizer=self.tokenizer,\r\n",
        "            max_length=self.hparams.max_length\r\n",
        "        )\r\n",
        "        loader = DataLoader(\r\n",
        "            dataset=dataset,\r\n",
        "            batch_size=self.hparams.val_batch_size\r\n",
        "        )\r\n",
        "        return loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiLo8XuGDIIy"
      },
      "source": [
        "def main():\r\n",
        "    # parser = argparse.ArgumentParser()\r\n",
        "\r\n",
        "    # parser.add_argument('data_dir')\r\n",
        "    # parser.add_argument('output_dir')\r\n",
        "\r\n",
        "    # parser.add_argument('--seed', default=42, type=int)\r\n",
        "\r\n",
        "    # parser.add_argument('--label_smoothing', default=0.1, type=float)\r\n",
        "    # parser.add_argument('--weight_decay', default=0.01, type=float)\r\n",
        "    # parser.add_argument('--lr', default=3e-5, type=float)\r\n",
        "    # parser.add_argument('--adam_betas', default='(0.9,0.999)')\r\n",
        "    # parser.add_argument('--adam_eps', default=1e-8, type=float)\r\n",
        "    # parser.add_argument('--num_warmup_steps', default=500, type=int)\r\n",
        "\r\n",
        "    # parser.add_argument('--train_batch_size', default=16, type=int)\r\n",
        "    # parser.add_argument('--val_batch_size', default=16, type=int)\r\n",
        "    # parser.add_argument('--max_length', default=128, type=int)\r\n",
        "\r\n",
        "    # parser.add_argument('--accumulate_grad_batches', default=4, type=int)\r\n",
        "    # parser.add_argument('--gpus', default=1, type=int)\r\n",
        "    # parser.add_argument('--gradient_clip_val', default=0.1, type=float)\r\n",
        "    # parser.add_argument('--max_epochs', default=16, type=int)\r\n",
        "\r\n",
        "    # args = parser.parse_args()\r\n",
        "\r\n",
        "    args = argparse.Namespace(\r\n",
        "        data_dir='stc-jpn',\r\n",
        "        output_dir='mt5_stc-jpn',\r\n",
        "        seed=42,\r\n",
        "        label_smoothing=0.1,\r\n",
        "        weight_decay=0.1,\r\n",
        "        lr=1e-4,\r\n",
        "        adam_betas='(0.9,0.999)',\r\n",
        "        adam_eps=1e-8,\r\n",
        "        num_warmup_steps=1,\r\n",
        "        train_batch_size=1,\r\n",
        "        val_batch_size=1,\r\n",
        "        max_length=16,\r\n",
        "        accumulate_grad_batches=1,\r\n",
        "        gpus=1,\r\n",
        "        gradient_clip_val=0.1,\r\n",
        "        max_epochs=4\r\n",
        "    )\r\n",
        "\r\n",
        "    if os.path.isdir(args.output_dir):\r\n",
        "        shutil.rmtree(args.output_dir)\r\n",
        "    os.mkdir(args.output_dir)\r\n",
        "\r\n",
        "    checkpoint_callback = ModelCheckpoint(\r\n",
        "        monitor='val_loss',\r\n",
        "        mode='min',\r\n",
        "        dirpath=args.output_dir\r\n",
        "    )\r\n",
        "    trainer = Trainer(\r\n",
        "        callbacks=[checkpoint_callback],\r\n",
        "        gradient_clip_val=args.gradient_clip_val,\r\n",
        "        gpus=args.gpus,\r\n",
        "        accumulate_grad_batches=args.accumulate_grad_batches,\r\n",
        "        max_epochs=args.max_epochs,\r\n",
        "        logger=[]\r\n",
        "    )\r\n",
        "\r\n",
        "    model = MT5Trainer(args)\r\n",
        "\r\n",
        "    trainer.fit(model)\r\n",
        "    trainer.test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfCqCuX1DL9V"
      },
      "source": [
        "if __name__ == '__main__':\r\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-2Y2kd9qSVx"
      },
      "source": [
        "copy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyqP94MohM2r"
      },
      "source": [
        "!cp mt5_stc-jpn/preds.txt drive/My\\ Drive/stc-jpn"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}